{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.0"
    },
    "latex_envs": {
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 0
    },
    "colab": {
      "name": "Finding Lane Lines on the Road.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambuj501/Google_Colab_Code/blob/master/Finding_Lane_Lines_on_the_Road.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9r2KnBAPbva",
        "colab_type": "text"
      },
      "source": [
        "# Finding Lane Lines on the Road\n",
        "***\n",
        "\n",
        "In this project, I used Python and OpenCV to find lane lines in the road images.  \n",
        "\n",
        "The following techniques are used:\n",
        "\n",
        "- Color Selection\n",
        "- Canny Edge Detection\n",
        "- Region of Interest Selection\n",
        "- Hough Transform Line Detection\n",
        "\n",
        "Finally, I applied all the techniques to process video clips to find lane lines in them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8llkG2kfOt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3iYnfflkedm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un9A8cJjPbvd",
        "colab_type": "text"
      },
      "source": [
        "## Loading Test Images\n",
        "\n",
        "The following libraries are used.  `cv2` is for OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2016-12-02T16:49:36.582538",
          "start_time": "2016-12-02T16:49:34.592060"
        },
        "id": "VUXXOwKXPbvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tunKSFGzPbvt",
        "colab_type": "text"
      },
      "source": [
        "The following `show_image` function is used to display images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTMhB9qGPbvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_images(images, cmap=None):\n",
        "    cols = 2\n",
        "    rows = (len(images)+1)//cols\n",
        "    \n",
        "    plt.figure(figsize=(10, 11))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        # use gray scale color map if there is only one channel\n",
        "        cmap = 'gray' if len(image.shape)==2 else cmap\n",
        "        plt.imshow(image, cmap=cmap)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
        "    plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlNG7wcvPbv8",
        "colab_type": "text"
      },
      "source": [
        "Let's load and examine the test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AecniLNPbv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af87ac75-1a73-4030-aebb-af8eef18c81d"
      },
      "source": [
        "test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')]\n",
        "\n",
        "show_images(test_images)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFba4J7iPbwJ",
        "colab_type": "text"
      },
      "source": [
        "Lines are in white or yellow.  A white lane is a series of alternating dots and short lines, which we need to detect as one line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPDLM1_FPbwL",
        "colab_type": "text"
      },
      "source": [
        "## Color Selection\n",
        "\n",
        "### RGB Color Space\n",
        "\n",
        "The images are loaded in RGB color space.  Let's try selecting only yellow and white colors in the images using the RGB channels.\n",
        "\n",
        "Reference: [RGB Color Code Chart](http://www.rapidtables.com/web/color/RGB_Color.htm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPOF97WFPbwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ef0c7cf-0cf0-4cf6-86bb-8b2bf9ddf3f2"
      },
      "source": [
        "# image is expected be in RGB color space\n",
        "def select_rgb_white_yellow(image): \n",
        "    # white color mask\n",
        "    lower = np.uint8([200, 200, 200])\n",
        "    upper = np.uint8([255, 255, 255])\n",
        "    white_mask = cv2.inRange(image, lower, upper)\n",
        "    # yellow color mask\n",
        "    lower = np.uint8([190, 190,   0])\n",
        "    upper = np.uint8([255, 255, 255])\n",
        "    yellow_mask = cv2.inRange(image, lower, upper)\n",
        "    # combine the mask\n",
        "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "    masked = cv2.bitwise_and(image, image, mask = mask)\n",
        "    return masked\n",
        "\n",
        "show_images(list(map(select_rgb_white_yellow, test_images)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGe0lgDtPbwV",
        "colab_type": "text"
      },
      "source": [
        "It looks pretty good except the last two.  The yellow lines are not clear due to the dark shade from the tree on the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyYcNdq8PbwX",
        "colab_type": "text"
      },
      "source": [
        "### HSL and HSV Color Space\n",
        "\n",
        "Using `cv2.cvtColor`, we can convert RGB image into different color space.  For example, [HSL and HSV color space](https://en.wikipedia.org/wiki/HSL_and_HSV).\n",
        "\n",
        "<img src='images/hsl-hsv.png' width='50%'>\n",
        "\n",
        "Image Source: [https://commons.wikimedia.org/wiki/File:Hsl-hsv_models.svg](https://commons.wikimedia.org/wiki/File:Hsl-hsv_models.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA3vXzzJPbwe",
        "colab_type": "text"
      },
      "source": [
        "### HSV Color Space\n",
        "\n",
        "How does it look when RGB images are converted into HSV color space?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Z7Ev1WPbwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89d50998-9034-49d3-f6e2-21317eb0cb15"
      },
      "source": [
        "def convert_hsv(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "show_images(list(map(convert_hsv, test_images)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wEeGXWHPbwo",
        "colab_type": "text"
      },
      "source": [
        "The yellow lines are very clear including the ones under the shades but the white lines are less clear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8pTY828Pbwq",
        "colab_type": "text"
      },
      "source": [
        "### HSL Color Space\n",
        "\n",
        "How does it look like when images are converted from RGB to HSL color space?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHC2qSbmPbwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b11981b3-b261-4bc2-d7f0-391f9b96e31a"
      },
      "source": [
        "def convert_hls(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
        "\n",
        "show_images(list(map(convert_hls, test_images)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrJ-J2S_Pbwy",
        "colab_type": "text"
      },
      "source": [
        "Both the white and yellow lines are clearly recognizable.  Also, the yellow lines under the shades are clearly shown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1pWgtJJPbwz",
        "colab_type": "text"
      },
      "source": [
        "Let's build a filter to select those white and yellow lines.  I want to select particular range of each channels (Hue, Saturation and Light).\n",
        "\n",
        "- Use `cv2.inRange` to filter the white color and the yellow color seperately.  \n",
        "  The function returns 255 when the filter conditon is satisfied.  Otherwise, it returns 0.\n",
        "- Use `cv2.bitwise_or` to combine these two binary masks.  \n",
        "  The combined mask returns 255 when either white or yellow color is detected.\n",
        "- Use `cv2.bitwise_and` to apply the combined mask onto the original RGB image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG62KY1EPbw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1ca1ad6-713e-4fa7-8910-99a3558b4c22"
      },
      "source": [
        "def select_white_yellow(image):\n",
        "    converted = convert_hls(image)\n",
        "    # white color mask\n",
        "    lower = np.uint8([  0, 200,   0])\n",
        "    upper = np.uint8([255, 255, 255])\n",
        "    white_mask = cv2.inRange(converted, lower, upper)\n",
        "    # yellow color mask\n",
        "    lower = np.uint8([ 10,   0, 100])\n",
        "    upper = np.uint8([ 40, 255, 255])\n",
        "    yellow_mask = cv2.inRange(converted, lower, upper)\n",
        "    # combine the mask\n",
        "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "    return cv2.bitwise_and(image, image, mask = mask)\n",
        "\n",
        "white_yellow_images = list(map(select_white_yellow, test_images))\n",
        "\n",
        "show_images(white_yellow_images)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxfOxCNPbw6",
        "colab_type": "text"
      },
      "source": [
        "For the white color, \n",
        "- I chose high **Light** value.\n",
        "- I did not filter **Hue**, **Saturation** values.\n",
        "  \n",
        "For the yellow color, \n",
        "- I chose **Hue** around 30 to choose yellow color.\n",
        "- I chose relatively high **Saturation** to exclude yellow hills\n",
        "\n",
        "The combined mask filters the yellow and white lines very clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLlIKQsBPbw-",
        "colab_type": "text"
      },
      "source": [
        "## Canny Edge Detection\n",
        "\n",
        "The Canny edge detector was developed by John F. Canny in 1986.  \n",
        "\n",
        "We want to detect edges in order to find straight lines especially lane lines.  For this, \n",
        "\n",
        "- use `cv2.cvtColor` to convert images into gray scale\n",
        "- use `cv2.GaussianBlur` to smooth out rough edges \n",
        "- use `cv2.Canny` to find edges\n",
        "\n",
        "Let's take a look at each step in details.\n",
        "\n",
        "Note: [Canny Edge Detection Wikipedia](https://en.wikipedia.org/wiki/Canny_edge_detector) has a good description in good details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEz21qqKPbxA",
        "colab_type": "text"
      },
      "source": [
        "### Gray Scaling\n",
        "\n",
        "The images should be converted into gray scaled ones in order to detect shapes (edges) in the images.  This is because the Canny edge detection measures the magnitude of pixel intensity changes or gradients (more on this later).\n",
        "\n",
        "Here, I'm converting the white and yellow line images from the above into gray scale for edge detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRf983s_PbxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b6b743e-f38b-4a6e-d934-72b3e3840aba"
      },
      "source": [
        "def convert_gray_scale(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "gray_images = list(map(convert_gray_scale, white_yellow_images))\n",
        "\n",
        "show_images(gray_images)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZsHJPsrPbxH",
        "colab_type": "text"
      },
      "source": [
        "### Gaussian Smoothing (Gaussian Blur)\n",
        "\n",
        "When there is an edge (i.e. a line), the pixel intensity changes rapidly (i.e. from 0 to 255) which we want to detect.  But before doing so, we should make the edges smoother.  As you can see,  the above images have many rough edges which causes many noisy edges to be detected.\n",
        "\n",
        "I use `cv2.GaussianBlur` to smooth out edges.\n",
        "\n",
        "- [Gaussian Filter OpenCV Theory](http://docs.opencv.org/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.html#gaussian-filter)\n",
        "- [cv2.GaussianBlur OpenCV API Reference](http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi8Df_zLPbxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_smoothing(image, kernel_size=15):\n",
        "    \"\"\"\n",
        "    kernel_size must be postivie and odd\n",
        "    \"\"\"\n",
        "    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teH_RJSbPbxP",
        "colab_type": "text"
      },
      "source": [
        "The GaussianBlur takes a `kernel_size` parameter which you'll need to play with to find one that works best.  I tried 3, 5, 9, 11, 15, 17 (they must be positive and odd) and check the edge detection (see the next section) result.  The bigger the `kernel_size` value is,  the more blurry the image becomes.  \n",
        "\n",
        "The bigger `kearnel_size` value requires more time to process. It is not noticeable with the test images but we should keep that in mind (later we'll be processing video clips).  So, we should prefer smaller values if the effect is similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oirdyZkBPbxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bce607f-3105-4f95-e83f-f46eea8432ec"
      },
      "source": [
        "blurred_images = list(map(lambda image: apply_smoothing(image), gray_images))\n",
        "\n",
        "show_images(blurred_images)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaqR8R4PPbxW",
        "colab_type": "text"
      },
      "source": [
        "### Edge Detection\n",
        "\n",
        "`cv2.Canny` takes two threshold values which requires some explanation.\n",
        "\n",
        "Wikipedia says:\n",
        "\n",
        "> it is essential to filter out the edge pixel with the weak gradient value and preserve the edge with the high gradient value. Thus two threshold values are set to clarify the different types of edge pixels, one is called high threshold value and the other is called the low threshold value. If the edge pixel’s gradient value is higher than the high threshold value, they are marked as strong edge pixels. If the edge pixel’s gradient value is smaller than the high threshold value and larger than the low threshold value, they are marked as weak edge pixels. If the pixel value is smaller than the low threshold value, they will be suppressed. \n",
        "\n",
        "According to the OpenCV documentation, the double thresholds are used as follows:\n",
        "\n",
        "- If a pixel gradient is higher than the upper threshold, the pixel is accepted as an edge\n",
        "- If a pixel gradient value is below the lower threshold, then it is rejected.\n",
        "- If the pixel gradient is between the two thresholds, then it will be accepted only if it is connected to a pixel that is above the upper threshold.\n",
        "- Canny recommended a upper:lower ratio between 2:1 and 3:1.\n",
        "\n",
        "These two threshold values are empirically determined.  Basically, you will need to define them by trials and errors.\n",
        "\n",
        "I first set the `low_threshold` to zero and then adjust the `high_threshold`.   If `high_threshold` is too high, you find no edges.  If `high_threshold` is too low, you find too many edges.  Once you find a good `high_threshold`, adjust the `low_threshold` to discard the weak edges (noises) connected to the strong edges.\n",
        "\n",
        "- [Canny Edge Detection OpenCV Theory](http://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)\n",
        "- [cv2.Canny OpenCV API Reference](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud88XspzPbxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0597e283-f152-489a-c58a-88857ad8c522"
      },
      "source": [
        "def detect_edges(image, low_threshold=50, high_threshold=150):\n",
        "    return cv2.Canny(image, low_threshold, high_threshold)\n",
        "\n",
        "edge_images = list(map(lambda image: detect_edges(image), blurred_images))\n",
        "\n",
        "show_images(edge_images)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-WTlEdlPbxe",
        "colab_type": "text"
      },
      "source": [
        "## Region of Interest Selection\n",
        "\n",
        "When finding lane lines, we don't need to check the sky and the hills.  \n",
        "\n",
        "Roughly speaking, we are interested in the aread surrounded by the red lines below:\n",
        "\n",
        "<img src='images/region-of-interest.png' width='50%'/>\n",
        "\n",
        "So, we exclude outside the region of interest by apply a mask.\n",
        "\n",
        "- [cv2.fillPoly OpenCV API Reference](http://docs.opencv.org/modules/core/doc/drawing_functions.html#fillpoly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHy3zL_1Pbxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90a98d99-fa52-4dbb-f15f-fc6f0f96f0d5"
      },
      "source": [
        "def filter_region(image, vertices):\n",
        "    \"\"\"\n",
        "    Create the mask using the vertices and apply it to the input image\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(image)\n",
        "    if len(mask.shape)==2:\n",
        "        cv2.fillPoly(mask, vertices, 255)\n",
        "    else:\n",
        "        cv2.fillPoly(mask, vertices, (255,)*mask.shape[2]) # in case, the input image has a channel dimension        \n",
        "    return cv2.bitwise_and(image, mask)\n",
        "\n",
        "    \n",
        "def select_region(image):\n",
        "    \"\"\"\n",
        "    It keeps the region surrounded by the `vertices` (i.e. polygon).  Other area is set to 0 (black).\n",
        "    \"\"\"\n",
        "    # first, define the polygon by vertices\n",
        "    rows, cols = image.shape[:2]\n",
        "    bottom_left  = [cols*0.1, rows*0.95]\n",
        "    top_left     = [cols*0.4, rows*0.6]\n",
        "    bottom_right = [cols*0.9, rows*0.95]\n",
        "    top_right    = [cols*0.6, rows*0.6] \n",
        "    # the vertices are an array of polygons (i.e array of arrays) and the data type must be integer\n",
        "    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
        "    return filter_region(image, vertices)\n",
        "\n",
        "\n",
        "# images showing the region of interest only\n",
        "roi_images = list(map(select_region, edge_images))\n",
        "\n",
        "show_images(roi_images)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oqlvqv1Pbxj",
        "colab_type": "text"
      },
      "source": [
        "Now we have lane lines but we need to recognize them as lines.  Especially, two lines: the left lane and the right lane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RUkBeMEPbxk",
        "colab_type": "text"
      },
      "source": [
        "## Hough Transform Line Detection\n",
        "\n",
        "I'm using `cv2.HoughLinesP` to detect lines in the edge images.\n",
        "\n",
        "There are several parameters you'll need to tweak and tune:\n",
        "\n",
        "- rho – Distance resolution of the accumulator in pixels.\n",
        "- theta – Angle resolution of the accumulator in radians.\n",
        "- threshold – Accumulator threshold parameter. Only those lines are returned that get enough votes (> `threshold`).\n",
        "- minLineLength – Minimum line length. Line segments shorter than that are rejected.\n",
        "- maxLineGap – Maximum allowed gap between points on the same line to link them.\n",
        "\n",
        "More details can be found:\n",
        "\n",
        "- [Hough Line Transform OpenCV Theory](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html)\n",
        "- [cv.HoughLinesP OpenCV API Reference](http://docs.opencv.org/modules/imgproc/doc/feature_detection.html?highlight=houghlinesp#houghlinesp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPtqPf60Pbxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hough_lines(image):\n",
        "    \"\"\"\n",
        "    `image` should be the output of a Canny transform.\n",
        "    \n",
        "    Returns hough lines (not the image with lines)\n",
        "    \"\"\"\n",
        "    return cv2.HoughLinesP(image, rho=1, theta=np.pi/180, threshold=20, minLineLength=20, maxLineGap=300)\n",
        "\n",
        "\n",
        "list_of_lines = list(map(hough_lines, roi_images))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQsB-JEMPbxr",
        "colab_type": "text"
      },
      "source": [
        "`list_of_lines` contains a list of lines detected.  With the above parameters, approximately 5-15 lines are detected for each image.\n",
        "\n",
        "Let's draw the lines onto the original images.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNm1aB-BPbxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7b46095-c659-4450-960a-c52ece8a4769"
      },
      "source": [
        "def draw_lines(image, lines, color=[255, 0, 0], thickness=2, make_copy=True):\n",
        "    # the lines returned by cv2.HoughLinesP has the shape (-1, 1, 4)\n",
        "    if make_copy:\n",
        "        image = np.copy(image) # don't want to modify the original\n",
        "    for line in lines:\n",
        "        for x1,y1,x2,y2 in line:\n",
        "            cv2.line(image, (x1, y1), (x2, y2), color, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "line_images = []\n",
        "for image, lines in zip(test_images, list_of_lines):\n",
        "    line_images.append(draw_lines(image, lines))\n",
        "    \n",
        "show_images(line_images)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxMzTuRfPbxw",
        "colab_type": "text"
      },
      "source": [
        "### Averaging and Extrapolating Lines\n",
        "\n",
        "There are multiple lines detected for a lane line.  We should come up with an averaged line for that.\n",
        "\n",
        "Also, some lane lines are only partially recognized.  We should extrapolate the line to cover full lane line length.\n",
        "\n",
        "We want two lane lines: one for the left and the other for the right.  The left lane should have a positive slope, and the right lane should have a negative slope.  Therefore, we'll collect positive slope lines and negative slope lines separately and take averages.\n",
        "\n",
        "Note: in the image, `y` coordinate is reversed.  The higher `y` value is actually lower in the image.  Therefore, the slope is negative for the left lane, and the slope is positive for the right lane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbrQjVB-Pbxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_slope_intercept(lines):\n",
        "    left_lines    = [] # (slope, intercept)\n",
        "    left_weights  = [] # (length,)\n",
        "    right_lines   = [] # (slope, intercept)\n",
        "    right_weights = [] # (length,)\n",
        "    \n",
        "    for line in lines:\n",
        "        for x1, y1, x2, y2 in line:\n",
        "            if x2==x1:\n",
        "                continue # ignore a vertical line\n",
        "            slope = (y2-y1)/(x2-x1)\n",
        "            intercept = y1 - slope*x1\n",
        "            length = np.sqrt((y2-y1)**2+(x2-x1)**2)\n",
        "            if slope < 0: # y is reversed in image\n",
        "                left_lines.append((slope, intercept))\n",
        "                left_weights.append((length))\n",
        "            else:\n",
        "                right_lines.append((slope, intercept))\n",
        "                right_weights.append((length))\n",
        "    \n",
        "    # add more weight to longer lines    \n",
        "    left_lane  = np.dot(left_weights,  left_lines) /np.sum(left_weights)  if len(left_weights) >0 else None\n",
        "    right_lane = np.dot(right_weights, right_lines)/np.sum(right_weights) if len(right_weights)>0 else None\n",
        "    \n",
        "    return left_lane, right_lane # (slope, intercept), (slope, intercept)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7APQSnn7Pbx3",
        "colab_type": "text"
      },
      "source": [
        "Using the above `average_lines` function, we can calculate average slope and intercept for the left and right lanes of each image.  \n",
        "\n",
        "Let's draw the lanes.  I need to convert the slope and intercept into pixel points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q4bX10cPbx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_line_points(y1, y2, line):\n",
        "    \"\"\"\n",
        "    Convert a line represented in slope and intercept into pixel points\n",
        "    \"\"\"\n",
        "    if line is None:\n",
        "        return None\n",
        "    \n",
        "    slope, intercept = line\n",
        "    \n",
        "    # make sure everything is integer as cv2.line requires it\n",
        "    x1 = int((y1 - intercept)/slope)\n",
        "    x2 = int((y2 - intercept)/slope)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    \n",
        "    return ((x1, y1), (x2, y2))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsS2OpvvPbx8",
        "colab_type": "text"
      },
      "source": [
        "Our `draw_lines` except a list of lines as the second parameter.  Each line is a list of 4 values (x1, y1, x2, y2).  The data type needs to be integer for `cv2.line` to work without throwing an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NzjtUlSPbx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95799e3b-2df6-4b9d-8271-9f6813b2a2fe"
      },
      "source": [
        "def lane_lines(image, lines):\n",
        "    left_lane, right_lane = average_slope_intercept(lines)\n",
        "    \n",
        "    y1 = image.shape[0] # bottom of the image\n",
        "    y2 = y1*0.6         # slightly lower than the middle\n",
        "\n",
        "    left_line  = make_line_points(y1, y2, left_lane)\n",
        "    right_line = make_line_points(y1, y2, right_lane)\n",
        "    \n",
        "    return left_line, right_line\n",
        "\n",
        "    \n",
        "def draw_lane_lines(image, lines, color=[255, 0, 0], thickness=20):\n",
        "    # make a separate image to draw lines and combine with the orignal later\n",
        "    line_image = np.zeros_like(image)\n",
        "    for line in lines:\n",
        "        if line is not None:\n",
        "            cv2.line(line_image, *line,  color, thickness)\n",
        "    # image1 * α + image2 * β + λ\n",
        "    # image1 and image2 must be the same shape.\n",
        "    return cv2.addWeighted(image, 1.0, line_image, 0.95, 0.0)\n",
        "             \n",
        "    \n",
        "lane_images = []\n",
        "for image, lines in zip(test_images, list_of_lines):\n",
        "    lane_images.append(draw_lane_lines(image, lane_lines(image, lines)))\n",
        "\n",
        "    \n",
        "show_images(lane_images)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x792 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfVgdzCEPbyC",
        "colab_type": "text"
      },
      "source": [
        "## Video Clips\n",
        "\n",
        "I'm drwaing lanes on video clips."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2016-12-02T16:49:43.976375",
          "start_time": "2016-12-02T16:49:43.965842"
        },
        "id": "gXUuSlp0PbyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "QUEUE_LENGTH=50\n",
        "\n",
        "class LaneDetector:\n",
        "    def __init__(self):\n",
        "        self.left_lines  = deque(maxlen=QUEUE_LENGTH)\n",
        "        self.right_lines = deque(maxlen=QUEUE_LENGTH)\n",
        "\n",
        "    def process(self, image):\n",
        "        white_yellow = select_white_yellow(image)\n",
        "        gray         = convert_gray_scale(white_yellow)\n",
        "        smooth_gray  = apply_smoothing(gray)\n",
        "        edges        = detect_edges(smooth_gray)\n",
        "        regions      = select_region(edges)\n",
        "        lines        = hough_lines(regions)\n",
        "        left_line, right_line = lane_lines(image, lines)\n",
        "\n",
        "        def mean_line(line, lines):\n",
        "            if line is not None:\n",
        "                lines.append(line)\n",
        "\n",
        "            if len(lines)>0:\n",
        "                line = np.mean(lines, axis=0, dtype=np.int32)\n",
        "                line = tuple(map(tuple, line)) # make sure it's tuples not numpy array for cv2.line to work\n",
        "            return line\n",
        "\n",
        "        left_line  = mean_line(left_line,  self.left_lines)\n",
        "        right_line = mean_line(right_line, self.right_lines)\n",
        "\n",
        "        return draw_lane_lines(image, (left_line, right_line))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYM_BB2bPbyG",
        "colab_type": "text"
      },
      "source": [
        "Let's try the one with the solid white lane on the right first ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2016-12-02T16:50:04.195890",
          "start_time": "2016-12-02T16:49:43.981206"
        },
        "scrolled": false,
        "id": "9-koH93ZPbyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_video(video_input, video_output):\n",
        "    detector = LaneDetector()\n",
        "\n",
        "    clip = VideoFileClip(os.path.join('test_videos', video_input))\n",
        "    processed = clip.fl_image(detector.process)\n",
        "    processed.write_videofile(os.path.join('output_videos', video_output), audio=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2016-12-02T16:50:55.379031",
          "start_time": "2016-12-02T16:50:04.217967"
        },
        "scrolled": false,
        "id": "18SCfNcOPbyK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ee736f77-24a2-427d-b503-2f525c404a4a"
      },
      "source": [
        "%time process_video('solidWhiteRight.mp4', 'white.mp4')    "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fd8688fd0899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time process_video('solidWhiteRight.mp4', 'white.mp4')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e339e47a2963>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_input, video_output)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLaneDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                                          fps_source=fps_source)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0;32m---> 33\u001b[0;31m                                    fps_source)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    270\u001b[0m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                       \"path.\")%filename)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file test_videos/solidWhiteRight.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48NdgwRCPbyO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "8c26229f-b4f9-4748-9bb7-5f17399b7843"
      },
      "source": [
        "%time process_video('solidYellowLeft.mp4', 'yellow.mp4')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7fca3497b7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time process_video('solidYellowLeft.mp4', 'yellow.mp4')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e339e47a2963>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_input, video_output)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLaneDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                                          fps_source=fps_source)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0;32m---> 33\u001b[0;31m                                    fps_source)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    270\u001b[0m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                       \"path.\")%filename)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file test_videos/solidYellowLeft.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjRyGSWfPbyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "acfd34c6-20c2-4e97-a204-ef19bda78779"
      },
      "source": [
        "%time process_video('challenge.mp4', 'extra.mp4')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7f346db88960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time process_video('challenge.mp4', 'extra.mp4')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e339e47a2963>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_input, video_output)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLaneDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                                          fps_source=fps_source)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0;32m---> 33\u001b[0;31m                                    fps_source)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    270\u001b[0m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                       \"path.\")%filename)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file test_videos/challenge.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfM_eT3XPbyU",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The project was successful in that the video images clearly show the lane lines are detected properly and lines are very smoothly handled.\n",
        "\n",
        "It only detects the straight lane lines.  It is an advanced topic to handle curved lanes (or the curvature of lanes).  We'll need to use perspective transformation and also poly fitting lane lines rather than fitting to straight lines.\n",
        "\n",
        "Having said that, the lanes near the car are mostly straight in the images.  The curvature appears at further distance unless it's a steep curve.  So, this basic lane finding technique is still very useful.\n",
        "\n",
        "Another thing is that it won't work for steep (up or down) roads because the region of interest mask is assumed from the center of the image.\n",
        "\n",
        "For steep roads, we first need to detect the holizontal line (between the sky and the earth) so that we can tell up to where the lines should extend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FOd12hFPlZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}